{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6.2 Pré-processamento completo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "I6tbbvCGLonl",
        "q6cTaLrAMgcs",
        "eG5gH3YbS7ir",
        "P4is3tc3UDF2",
        "8RtSUTCrVzGk",
        "FsMqeFL9WVN8"
      ],
      "authorship_tag": "ABX9TyNjwOmBS1vODDLqUxPbEVc5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWxeJ2jSIIp5"
      },
      "source": [
        "#Extração de Features\n",
        "\n",
        "Para representar dados textuais de forma que os mesmos possam ser usados por algoritmos de aprendizado de máquina, devemos representá-los numericamente. Mas antes de usarmos alguma técnica de representação devemos passar os dados por um pré-processamento com as etapas como tokenização, stemming, lematização e remoção de stopwords.\n",
        "\n",
        "Dados podem ser divididos em três categorias com base na sua estrutura: estruturados, semi-estruturados e não estruturados.Mas os dados também podem ser dividios em quatro categorias com base no seu conteúdo: texto, imagem, áudio e vídeo. \n",
        "\n",
        "Sentenças escritas são uma forma de dados não estruturados com conteúdo de texto. Na maioria das vezes esses dados não podem ser usados da maneira que estão, pois possuem muitos elementos ruidosos. Por isso, é recomendado passar eles por processos de pré-processamento para que realizemos análises que obtenham resultados mais precisos.\n",
        "\n",
        "Nesse caderno realizaremos a limpeza de dados textuais através de uma série de pré-processamentos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6tbbvCGLonl"
      },
      "source": [
        "## Limpeza de texto e tokenização\n",
        "\n",
        "Processo de retirada de stopwords e pontuação para fazer os tokens serem mais significativos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTcaLDUDLnC-"
      },
      "source": [
        "# Criando função que realizará a limpeza\n",
        "import re\n",
        "def limpeza(texto):\n",
        "  return re.sub(r'([^\\s\\w]|_)+', ' ', texto).split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3DlFdhHMFy7"
      },
      "source": [
        "# Nosso texto de trabalho será um tweet com emojis, pontuação e stopwords\n",
        "texto = 'Sunil tweeted, \"Witnessing 70th Republic Day of India from Rajpath, \\\n",
        "            New Delhi. Mesmerizing performance by Indian Army! Awesome airshow! @india_official \\\n",
        "            @indian_army #India #70thRepublic_Day. For more photos ping me sunil@photoking.com :)\"'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5nnwTKDMF3X",
        "outputId": "70ca9833-ef9b-4eb4-c8ac-bd6b9f4325e2"
      },
      "source": [
        "# Limpando o texto\n",
        "limpeza(texto)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sunil',\n",
              " 'tweeted',\n",
              " 'Witnessing',\n",
              " '70th',\n",
              " 'Republic',\n",
              " 'Day',\n",
              " 'of',\n",
              " 'India',\n",
              " 'from',\n",
              " 'Rajpath',\n",
              " 'New',\n",
              " 'Delhi',\n",
              " 'Mesmerizing',\n",
              " 'performance',\n",
              " 'by',\n",
              " 'Indian',\n",
              " 'Army',\n",
              " 'Awesome',\n",
              " 'airshow',\n",
              " 'india',\n",
              " 'official',\n",
              " 'indian',\n",
              " 'army',\n",
              " 'India',\n",
              " '70thRepublic',\n",
              " 'Day',\n",
              " 'For',\n",
              " 'more',\n",
              " 'photos',\n",
              " 'ping',\n",
              " 'me',\n",
              " 'sunil',\n",
              " 'photoking',\n",
              " 'com']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6cTaLrAMgcs"
      },
      "source": [
        "## Extração de n-gramas\n",
        "\n",
        "Extração de n-gramas através de três técnicas diferentes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aaUIOpDMxZu"
      },
      "source": [
        "# Criando função que extrai n-gramas com n como parâmetro\n",
        "import re\n",
        "def ext_n_grama(texto, n):\n",
        "  ngramas = []\n",
        "  tokens = limpeza(texto)\n",
        "  for i in range(len(tokens)-n+1):\n",
        "    ngramas.append(tokens[i:i+n])\n",
        "  return ngramas "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTiqsWCXNcoU",
        "outputId": "aa37c830-18b2-4213-8d3a-efc56d154fdc"
      },
      "source": [
        "# Teste com 2-gramas\n",
        "ext_n_grama('The cute little boy is playing with the kitten.', 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['The', 'cute'],\n",
              " ['cute', 'little'],\n",
              " ['little', 'boy'],\n",
              " ['boy', 'is'],\n",
              " ['is', 'playing'],\n",
              " ['playing', 'with'],\n",
              " ['with', 'the'],\n",
              " ['the', 'kitten']]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XEuAHPzNlcE",
        "outputId": "b9cc9deb-0e60-4a3e-cf00-4c47c61a328c"
      },
      "source": [
        "# Teste com 3-gramas\n",
        "ext_n_grama('The cute little boy is playing with the kitten.', 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['The', 'cute', 'little'],\n",
              " ['cute', 'little', 'boy'],\n",
              " ['little', 'boy', 'is'],\n",
              " ['boy', 'is', 'playing'],\n",
              " ['is', 'playing', 'with'],\n",
              " ['playing', 'with', 'the'],\n",
              " ['with', 'the', 'kitten']]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK1YVZRNNtq6"
      },
      "source": [
        "# Vamos usar agora o pacote nltk\n",
        "from nltk import ngrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjxu5S1tN0W8",
        "outputId": "5e1d1814-27cc-4c04-f714-33814d2ccca0"
      },
      "source": [
        "# Teste para 2-gramas\n",
        "list(ngrams(limpeza('The cute little boy is playing with the kitten.'), 2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'cute'),\n",
              " ('cute', 'little'),\n",
              " ('little', 'boy'),\n",
              " ('boy', 'is'),\n",
              " ('is', 'playing'),\n",
              " ('playing', 'with'),\n",
              " ('with', 'the'),\n",
              " ('the', 'kitten')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-Nm1asYN9Pj",
        "outputId": "d709b393-7eae-4f3c-ba5f-19982915a89e"
      },
      "source": [
        "# Teste para 3-gramas\n",
        "list(ngrams(limpeza('The cute little boy is playing with the kitten.'), 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'cute', 'little'),\n",
              " ('cute', 'little', 'boy'),\n",
              " ('little', 'boy', 'is'),\n",
              " ('boy', 'is', 'playing'),\n",
              " ('is', 'playing', 'with'),\n",
              " ('playing', 'with', 'the'),\n",
              " ('with', 'the', 'kitten')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCH1fb9pN-bM",
        "outputId": "515f7fcb-3d8b-480c-bf40-efd1aa10da7e"
      },
      "source": [
        "# Instalando e importando biblioteca textblob\n",
        "!pip install -U textblob\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Collecting textblob\n",
            "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n",
            "Installing collected packages: textblob\n",
            "  Attempting uninstall: textblob\n",
            "    Found existing installation: textblob 0.15.3\n",
            "    Uninstalling textblob-0.15.3:\n",
            "      Successfully uninstalled textblob-0.15.3\n",
            "Successfully installed textblob-0.17.1\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqgFYxRsOMU8"
      },
      "source": [
        "# Criando objeto blob com a frase\n",
        "blob = TextBlob(\"The cute little boy is playing with the kitten.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLzUaZGgSxJi",
        "outputId": "d2b74e9e-77cb-4945-b79a-51f55d0380cc"
      },
      "source": [
        "# Testando para 2-gramas\n",
        "blob.ngrams(n=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['The', 'cute']),\n",
              " WordList(['cute', 'little']),\n",
              " WordList(['little', 'boy']),\n",
              " WordList(['boy', 'is']),\n",
              " WordList(['is', 'playing']),\n",
              " WordList(['playing', 'with']),\n",
              " WordList(['with', 'the']),\n",
              " WordList(['the', 'kitten'])]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw7-_GZtS0uc",
        "outputId": "544e71a4-4f5e-48ec-eb38-e2941f8bcdcf"
      },
      "source": [
        "# Testando para 3-gramas\n",
        "blob.ngrams(n=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['The', 'cute', 'little']),\n",
              " WordList(['cute', 'little', 'boy']),\n",
              " WordList(['little', 'boy', 'is']),\n",
              " WordList(['boy', 'is', 'playing']),\n",
              " WordList(['is', 'playing', 'with']),\n",
              " WordList(['playing', 'with', 'the']),\n",
              " WordList(['with', 'the', 'kitten'])]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG5gH3YbS7ir"
      },
      "source": [
        "## Tokenização usando bibliotecas\n",
        "\n",
        "Agora, realizaremos a tokenização usando as bibliotecas Keras e TextBlob."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBYf_szWTPwZ"
      },
      "source": [
        "# Importando os módulos de ambas as bibliotecas usados na tokenização\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mySF-OKqTW2y"
      },
      "source": [
        "# Definindo a sentença com a qual trabalharemos\n",
        "sentenca = 'Sunil tweeted, \"Witnessing 70th Republic Day of India from Rajpath, \\\n",
        "New Delhi. Mesmerizing performancesby Indian Army! Awesome airshow! @india_official \\\n",
        "@indian_army #India #70thRepublic_Day. For more photos ping me sunil@photoking.com :)\"'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ApjHUQFTW40",
        "outputId": "dd5de8e4-aefd-47f3-fde7-87e925fcb8b7"
      },
      "source": [
        "# Criando função de tokenização usando Keras e testando\n",
        "def keras_tokens(texto):\n",
        "  return text_to_word_sequence(texto)\n",
        "keras_tokens(sentenca)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sunil',\n",
              " 'tweeted',\n",
              " 'witnessing',\n",
              " '70th',\n",
              " 'republic',\n",
              " 'day',\n",
              " 'of',\n",
              " 'india',\n",
              " 'from',\n",
              " 'rajpath',\n",
              " 'new',\n",
              " 'delhi',\n",
              " 'mesmerizing',\n",
              " 'performancesby',\n",
              " 'indian',\n",
              " 'army',\n",
              " 'awesome',\n",
              " 'airshow',\n",
              " 'india',\n",
              " 'official',\n",
              " 'indian',\n",
              " 'army',\n",
              " 'india',\n",
              " '70threpublic',\n",
              " 'day',\n",
              " 'for',\n",
              " 'more',\n",
              " 'photos',\n",
              " 'ping',\n",
              " 'me',\n",
              " 'sunil',\n",
              " 'photoking',\n",
              " 'com']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myehLE2jTy82",
        "outputId": "ae566d24-0b49-4fc4-f34e-b5246fd49096"
      },
      "source": [
        "# Criando função de tokenização usando TextBlob e testando\n",
        "def TextBlob_tokens(texto):\n",
        "  blob = TextBlob(texto)\n",
        "  return blob.words\n",
        "TextBlob_tokens(sentenca)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['Sunil', 'tweeted', 'Witnessing', '70th', 'Republic', 'Day', 'of', 'India', 'from', 'Rajpath', 'New', 'Delhi', 'Mesmerizing', 'performancesby', 'Indian', 'Army', 'Awesome', 'airshow', 'india_official', 'indian_army', 'India', '70thRepublic_Day', 'For', 'more', 'photos', 'ping', 'me', 'sunil', 'photoking.com'])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4is3tc3UDF2"
      },
      "source": [
        "## Removendo a conjugação das palavras\n",
        "\n",
        "Vamos remover a terminação -ing usando o RegexStemmer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6pOMUtoU-ku"
      },
      "source": [
        "# Importando função da NLTK que nos permitirá realizar a stemmização\n",
        "from nltk.stem import RegexpStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YssXv7EVFNt"
      },
      "source": [
        "# Definindo função que usaremos para obter os stems\n",
        "def cria_stems(texto):\n",
        "  # Definindo stemmer que remove ing para palavras com mais de 4 letras\n",
        "  stemmer = RegexpStemmer('ing$', min=4)\n",
        "  # Refazendo a sentença, mas sem a conjugação\n",
        "  return ' '.join([stemmer.stem(palavra) for palavra in texto.split()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "uMX2n_jRVo8Y",
        "outputId": "42fd7f6c-2c25-423f-afdc-af75a8d1ec4c"
      },
      "source": [
        "# Testando a função\n",
        "sentenca = \"I love playing football\"\n",
        "cria_stems(sentenca)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I love play football'"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RtSUTCrVzGk"
      },
      "source": [
        "## Usando stemmer de Porter\n",
        "\n",
        "Agora, vamos realizar o stem usando o stemmer de Porter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTV2wKEZV9xG"
      },
      "source": [
        "# Importando pacote necessário\n",
        "from nltk.stem.porter import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXmGnUj8WBev"
      },
      "source": [
        "# Definindo função que usaremos\n",
        "def cria_stems_porter(texto):\n",
        "  stemmer = PorterStemmer()\n",
        "  return ' '.join([stemmer.stem(palavra) for palavra in texto.split()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mRLLgIiEWP2-",
        "outputId": "363c3e8e-a169-489b-9441-c49c8de5468f"
      },
      "source": [
        "# Testando função\n",
        "texto = \"Before eating, it would be nice to sanitize your hands with a sanitizer\"\n",
        "cria_stems_porter(texto)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'befor eating, it would be nice to sanit your hand with a sanit'"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsMqeFL9WVN8"
      },
      "source": [
        "## Lematização\n",
        "\n",
        "Vamos realizar o processo de lematização"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2DbIGRCWfyP",
        "outputId": "33a0a485-0203-42e2-eab3-7e17c2466f87"
      },
      "source": [
        "# Importando dependências necessárias\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0armBGO2Wxy0"
      },
      "source": [
        "# Criando função de lematização\n",
        "def lematizador(texto):\n",
        "  lemmatizador = WordNetLemmatizer()\n",
        "  return ' '.join([lemmatizador.lemmatize(palavra) for palavra in word_tokenize(texto)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-rIkol_fW_e0",
        "outputId": "6e0e7428-08da-47b1-8ea8-c24b48215d70"
      },
      "source": [
        "# Testando função\n",
        "sentenca = \"The products produced by the process today are far better than what it produces generally.\"\n",
        "lematizador(sentenca)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The product produced by the process today are far better than what it produce generally .'"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MCfYhpEXIgN"
      },
      "source": [
        "## Singular e plural\n",
        "\n",
        "Criando funções que substituem por plural ou por singular."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5WX9DiMXFaU",
        "outputId": "0aefacfb-e26e-4261-a423-f54efe024f1f"
      },
      "source": [
        "# Importando dependências\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uva_0wo5XsBE"
      },
      "source": [
        "# Tokens com os quais trabalharemos\n",
        "sentenca = TextBlob('She sells seashells on the seashore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZF7_qL2XWrn",
        "outputId": "1370026f-c1c4-42f5-e667-e82cf3356eea"
      },
      "source": [
        "# Criando singularizador e testando\n",
        "def singularizador(palavra):\n",
        "    return palavra.singularize()\n",
        "for palavra in sentenca.words:\n",
        "  print(singularizador(palavra))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "She\n",
            "sell\n",
            "seashell\n",
            "on\n",
            "the\n",
            "seashore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wahX8ofYsOK",
        "outputId": "1ed62fe4-f935-4635-e39b-901c7f5e24a2"
      },
      "source": [
        "# Criando pluralizador e testando\n",
        "def pluralizador(palavra):\n",
        "    return palavra.pluralize()\n",
        "for palavra in sentenca.words:\n",
        "  print(pluralizador(palavra))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shes\n",
            "sellss\n",
            "seashellss\n",
            "ons\n",
            "thes\n",
            "seashores\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ArngTM7Y6ub"
      },
      "source": [
        "## Tradução em Python\n",
        "\n",
        "A biblioteca TextBlob nos fornece um recurso de tradução automática de uma sentença."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKk1f3OfZAly"
      },
      "source": [
        "# Importando tradutor\n",
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc2ukBwZZD0s"
      },
      "source": [
        "# Definindo função tradutora\n",
        "def tradutor(texto, from_l, to_l):\n",
        "    blob = TextBlob(texto)\n",
        "    return blob.translate(from_lang=from_l, to=to_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkvVdT7wZNaN",
        "outputId": "d8f9299c-4e78-4a06-bd9d-5e7813c6e68c"
      },
      "source": [
        "# Testando\n",
        "tradutor(texto='olá, bom dia', from_l='pt', to_l='en')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"Hello good Morning\")"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5HQPw8oZYRE"
      },
      "source": [
        "## Removendo stopwords\n",
        "\n",
        "Vamos remover as stopwords de um texto através da word_tokenizer da NLTK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXImvJxbZ0kf"
      },
      "source": [
        "# Importando dependências\n",
        "from nltk import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8TalCJmZ4Y6"
      },
      "source": [
        "# Definindo função de remoção de stopwords\n",
        "def remover_sw(texto,stopwords):\n",
        "    return ' '.join([palavra for palavra in word_tokenize(texto) if palavra.lower() not in stopwords])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "og7_LSoeaIsB",
        "outputId": "3516b112-5638-4ff3-f9f2-de71d1923fcb"
      },
      "source": [
        "# Testando\n",
        "sentenca = \"She sells seashells on the seashore\"\n",
        "stopwords = ['she', 'on', 'the', 'am', 'is', 'not']\n",
        "remover_sw(sentenca, stopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'sells seashells seashore'"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    }
  ]
}