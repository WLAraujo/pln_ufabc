{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenização, NLTK e analítica básica.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RDtLs85yP5s0",
        "RK8tuf2UV5fM",
        "3vyXSRtAV2QV",
        "bOyGgxKHYfNK",
        "8Wu85D84d_e_",
        "ysc6tup3gu3e",
        "3xtC4zdZ4YiB"
      ],
      "authorship_tag": "ABX9TyN41hHJKqVTLHCM0H54gpWU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WLAraujo/python_PLN/blob/main/3_tokens_nltk_analitica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDtLs85yP5s0"
      },
      "source": [
        "# Tokenização\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn3K-OGVAijO"
      },
      "source": [
        "Nesse caderno trabalharemos com os conceitos de tokenização e vocabulário aplicados em alguns textos. \n",
        "\n",
        "Lembrando que o processo de criação de **tokens e vocabulários** consiste em contar e identificar a quantidade e a diversidade das palavras em um texto. A quantidade de tokens em um texto é a quantidade de palavras que ele contém, já a quantidade de vocabulários de um texto é quantidade de palavras diferentes que ele contém.\n",
        "\n",
        "Também veremos a NLTK. O **Natural Language Toolkit**, ou mais comumente o **NLTK**, é um conjunto de bibliotecas e programas para processamento simbólico e estatístico da linguagem natural, escrito na linguagem de programação Python.\n",
        "\n",
        "Ele fornece interfaces fáceis de usar para mais de 50 corpora e recursos lexicais, como WordNet, junto com um conjunto de bibliotecas de processamento de texto para classificação, tokenização, lematização, marcação, análise e raciocínio semântico etc.\n",
        "\n",
        "Por fim, veremos sobre **analítica de texto** básica, lembrando que analítica de teto refere-se a métodos de extrair percepções significativas e responder perguntas sobre os dados do texto analisado. Já a PLN nos auxilia a entrar na parte de análise semântica do texto. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz5s9gYbEyhL"
      },
      "source": [
        "# Importando bibliotecas\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yagu2JpKAo1C"
      },
      "source": [
        "## 1. Tokenização de palavras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK8tuf2UV5fM"
      },
      "source": [
        "### 1.1 Texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bhwivY_ZrTf"
      },
      "source": [
        "# Vamos calcular quantos tokens o seguinte texto possui\n",
        "texto = (\"\"\"A capivara (nome científico: Hydrochoerus hydrochaeris) é uma espécie\n",
        "         de mamífero roedor da família Caviidae e subfamília Hydrochoerinae. \n",
        "         Alguns autores consideram que deva ser classificada em uma família própria. \n",
        "         Está incluída no mesmo grupo de roedores ao qual se classificam as pacas, \n",
        "         cutias, os preás e o porquinho-da-índia. Ocorre por toda a América do Sul \n",
        "         ao leste dos Andes em habitats associados a rios, lagos e pântanos, \n",
        "         do nível do mar até 1 300 m de altitude. Extremamente adaptável, \n",
        "         pode ocorrer em ambientes altamente alterados pelo ser humano. (d'água)\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ccxiedCIpTx"
      },
      "source": [
        "# Regex para identificar qualquer palavra\n",
        "regex = r\"[-'a-zA-ZÀ-ÖØ-öø-ÿ]+\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwQIL2eMI90s"
      },
      "source": [
        "# Construindo vetor de tokens através da função finditer que devolve um iterável\n",
        "tokens = re.findall(regex, texto)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khQDuzkbQDYz"
      },
      "source": [
        "# Definindo um conjunto para ser a estrutura de vocabulário\n",
        "# Lembrando que conjuntos em python não armazenam duas vezes os mesmo elemento\n",
        "vocabulario = set([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unNjunYiSkmL"
      },
      "source": [
        "# Adicionando palavras ao vocabulário\n",
        "for token in tokens:\n",
        "  vocabulario.add(token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKxuZy-6TYpW",
        "outputId": "667dde94-049b-4b6d-85ea-a55aae013126"
      },
      "source": [
        "# Verificando a quantidade de tokens e o tamanho do vocabulário\n",
        "print (f\"T={i+1}, V={len(vocabulario)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N=86, V=74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vyXSRtAV2QV"
      },
      "source": [
        "### 1.2 Livro"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N4Vw5YzWAuU"
      },
      "source": [
        "# Criando regex que será usada\n",
        "regex = r\"[-'a-zA-ZÀ-ÖØ-öø-ÿ0-9]+\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Si7Yq8tXKy3",
        "outputId": "815fc4c0-964e-4d87-a325-7c0e37af049a"
      },
      "source": [
        "# Abrindo o documento que será usado\n",
        "with open(\"/content/dom_casmurro.txt\",'r') as doc:\n",
        "  conteudo = doc.read()\n",
        "  tokens = re.findall(regex, conteudo)\n",
        "  print(f\"Total de tokens = {len(tokens)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de tokens = 69049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOyGgxKHYfNK"
      },
      "source": [
        "### 1.3 Frequência das palavras no livro"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF5quJOabFWh",
        "outputId": "ef2dd225-b89e-45b3-9275-ec7e4ed7fb3f"
      },
      "source": [
        "# Reaproveitando o regex do último item vamos calcular a frequência de cada palavra\n",
        "with open(\"/content/dom_casmurro.txt\",'r') as doc:\n",
        "    conteudo  = doc.read()\n",
        "    # Identificando as palavras usando um dicionário\n",
        "    palavras = re.findall(regex, conteudo)\n",
        "    frequencias = dict([])\n",
        "    # Contando a quantidade de vezes no documento através dos valores no dicionário\n",
        "    for palavra in palavras:\n",
        "        # Colocando as letras das palavras em minúsculo\n",
        "        palavra = palavra.lower()\n",
        "        # Verificando se a palavra já foi adicionada no dicionário\n",
        "        if palavra not in frequencias:\n",
        "            frequencias[palavra] = 0\n",
        "        frequencias[palavra] += 1\n",
        "    # Mostrando a quantidade de tokens e o tamanho do vocabulário\n",
        "    print (f\"Tokens: {len(palavras)}, Vocabulario: {len(frequencias)}\")\n",
        "    # Vendo as 20 palavras mais frequentes\n",
        "    freq_ord = sorted(frequencias, key=frequencias.get, reverse=True)\n",
        "    for i in range(0,20):\n",
        "        print (f\"--> {frequencias[freq_ord[i]]} : {freq_ord[i]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: 69049, Vocabulario: 10404\n",
            "--> 2635 : que\n",
            "--> 2558 : a\n",
            "--> 2194 : e\n",
            "--> 1975 : de\n",
            "--> 1704 : o\n",
            "--> 1428 : não\n",
            "--> 783 : um\n",
            "--> 698 : é\n",
            "--> 666 : os\n",
            "--> 640 : do\n",
            "--> 629 : da\n",
            "--> 567 : mas\n",
            "--> 562 : se\n",
            "--> 550 : era\n",
            "--> 547 : as\n",
            "--> 539 : para\n",
            "--> 535 : com\n",
            "--> 518 : eu\n",
            "--> 489 : me\n",
            "--> 462 : em\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wu85D84d_e_"
      },
      "source": [
        "### 1.4 Removendo stop words do texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqQp5kJ5d-f4",
        "outputId": "66a7b468-1f25-4063-9c0c-c6ee1944c97f"
      },
      "source": [
        "# Como vimos na nossa análise de frequência\n",
        "# a maioria das palavras mais frequentes são stopwords\n",
        "# vamos remover as stopwords da nossa análise\n",
        "\n",
        "# Também vamos reaproveitar a regex já definida\n",
        "\n",
        "# Leitura das stopwords de um arquivo\n",
        "stopwords = set([])\n",
        "with open(\"/content/stopwords_pt.txt\",'r') as stopwords_txt:\n",
        "    for linha in stopwords_txt.readlines():\n",
        "        stopwords.add(linha.strip().lower())\n",
        "\n",
        "# Repetindo o processo do último item mas removendo as stopwords\n",
        "with open(\"/content/dom_casmurro.txt\",'r') as doc:\n",
        "    conteudo  = doc.read()\n",
        "    palavras = re.findall(regex, conteudo)\n",
        "    frequencias = dict([])\n",
        "    for palavra in palavras:\n",
        "        palavra = palavra.lower()\n",
        "        # Verificando se a palavra não é uma stopword\n",
        "        if palavra not in stopwords:\n",
        "          if palavra not in frequencias:\n",
        "              frequencias[palavra] = 0\n",
        "          frequencias[palavra] += 1\n",
        "    print (f\"Tokens: {len(palavras)}, Vocabulario: {len(frequencias)}\")\n",
        "    freq_ord = sorted(frequencias, key=frequencias.get, reverse=True)\n",
        "    for i in range(0,20):\n",
        "        print (f\"--> {frequencias[freq_ord[i]]} : {freq_ord[i]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: 69049, Vocabulario: 10001\n",
            "--> 340 : capitú\n",
            "--> 262 : á\n",
            "--> 237 : elle\n",
            "--> 229 : mãe\n",
            "--> 191 : dias\n",
            "--> 188 : the\n",
            "--> 186 : ella\n",
            "--> 170 : casa\n",
            "--> 166 : olhos\n",
            "--> 162 : mim\n",
            "--> 158 : josé\n",
            "--> 123 : of\n",
            "--> 119 : disse\n",
            "--> 111 : padre\n",
            "--> 106 : escobar\n",
            "--> 104 : --não\n",
            "--> 97 : cousa\n",
            "--> 89 : ia\n",
            "--> 88 : project\n",
            "--> 85 : seminario\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysc6tup3gu3e"
      },
      "source": [
        "## 2. NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imjupDWwg9Ce"
      },
      "source": [
        "# Importando o NLTK\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MQ9OGi1hWCk"
      },
      "source": [
        "O NLTK possui uma série de pacotes adicionais ou corpora que podem ser facilmente adicionados à instalação básica da biblioteca.\n",
        "\n",
        "Um dos corpora disponibilizados pelo NLTK é a obra completa de Machado de Assis. O nome desse pacote é \"machado\".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q4q0skthbJd"
      },
      "source": [
        "# Importando o corpus de Machado de Assis\n",
        "from nltk.corpus import machado"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBNSOZgYhihV",
        "outputId": "9b8a80d4-fba7-462e-fb42-8203cdc97938"
      },
      "source": [
        "# Uma outra forma de baixar esse corpus seria através de\n",
        "nltk.download(\"machado\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]   Package machado is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfCGYKRwi4Bl"
      },
      "source": [
        "# Extraindo arquivos e mudando a pasta deles\n",
        "!unzip /root/nltk_data/corpora/machado.zip\n",
        "!mv /content/machado /root/nltk_data/corpora"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu4NAb2dhxzm",
        "outputId": "3c94e39f-f207-44c3-86ce-813e6b44da5d"
      },
      "source": [
        "# Obtendo algumas informações sobre o corpus\n",
        "print(f\"Número de arquivos no corpus: {len(machado.fileids())}\")\n",
        "print(f\"Primeiros cinco textos do corpus: {machado.fileids()[0:10]}\")\n",
        "print(f\"Quantas palavras existem nesse corpus: {len(machado.words())}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de arquivos no corpus: 246\n",
            "Primeiros cinco textos do corpus: ['contos/macn001.txt', 'contos/macn002.txt', 'contos/macn003.txt', 'contos/macn004.txt', 'contos/macn005.txt', 'contos/macn006.txt', 'contos/macn007.txt', 'contos/macn008.txt', 'contos/macn009.txt', 'contos/macn010.txt']\n",
            "Quantas palavras existem nesse corpus: 3121944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7EtrsltkMJC",
        "outputId": "0ed28d94-aa55-40da-b139-790e57367ac9"
      },
      "source": [
        "# Retornando um texto como uma lista de tokens através do método words\n",
        "texto_tk = machado.words('romance/marm05.txt')\n",
        "# Vendo os tokens e a quantidade deles\n",
        "print(texto_tk)\n",
        "print(len(texto_tk))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Romance', ',', 'Memórias', 'Póstumas', 'de', 'Brás', ...]\n",
            "77098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhdCeZmNw9Xb"
      },
      "source": [
        "# Uma outra forma de extrair essas informações é através da classe Text\n",
        "# Para usar a clase Text é necessário importá-la\n",
        "from nltk.text import Text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9bhtmH41chH",
        "outputId": "b4298558-b384-4f87-bbe8-06d54ece590d"
      },
      "source": [
        "# Com a classe Text podemos encontrar os tokens através da mesma função words\n",
        "bras = Text(machado.words('romance/marm05.txt'))\n",
        "bras[0:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Romance',\n",
              " ',',\n",
              " 'Memórias',\n",
              " 'Póstumas',\n",
              " 'de',\n",
              " 'Brás',\n",
              " 'Cubas',\n",
              " ',',\n",
              " '1880',\n",
              " 'Memórias']"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MVii1ey1l0x",
        "outputId": "3fa4154f-53a3-49f6-a668-7bcb6e7b24f4"
      },
      "source": [
        "# Além disso também podemos verificar quais os contextos que uma palavra se encontra\n",
        "# Para isso usamos a função concordande, passando a palavra desejada como parâmetro\n",
        "bras.concordance('praça')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 1 of 1 matches:\n",
            "era um preto que vergalhava outro na praça . O outro não se atrevia a fugir ; g\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJGdj4hY1_Ho",
        "outputId": "1b940aba-9a37-42f2-9c45-2f2d6c418914"
      },
      "source": [
        "# Também podemos remover as stopwords do português com a NLTK\n",
        "# Para fazer isso temos que baixar o pacote da NLTK \n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyD0Wpqd3814",
        "outputId": "0517fd62-e455-45c3-fd92-6c0c50fdae06"
      },
      "source": [
        "# Verificando algumas das stop words e a quantidade de stopwords\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "print(stopwords[:10])\n",
        "print(len(stopwords))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['de', 'a', 'o', 'que', 'e', 'é', 'do', 'da', 'em', 'um']\n",
            "204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xtC4zdZ4YiB"
      },
      "source": [
        "## 3. Analítica do texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMgn_Ii54sFk"
      },
      "source": [
        "Vamos fazer algumas funções que nos auxiliam a realizar análises de texto básicas nos baseando nos dados de alguns textos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-pVdbt04pyd"
      },
      "source": [
        "frase = 'The quick brown fox jumps over the lazy dog'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rMFzLsp5t4N"
      },
      "source": [
        "# Função que verifica se palavra pertence a uma sentença\n",
        "def encontra_palavra(palavra, frase):\n",
        "    return palavra in frase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5plIz4nl6Wcn",
        "outputId": "cd4dae5e-86b4-457b-a5de-ede5a88839ac"
      },
      "source": [
        "encontra_palavra('quick', frase)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX3V4SWd6jTu"
      },
      "source": [
        "# Função que retorna o índice em que uma palavra começa na frase\n",
        "def encontra_ind(palavra, texto):\n",
        "    return texto.index(palavra)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQyHesjL69Or",
        "outputId": "5c1e2869-a623-40bb-c260-e427eff2efb1"
      },
      "source": [
        "encontra_ind('fox', frase)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7kP1uc57SWc",
        "outputId": "7e6abde5-4cfa-4513-a6be-381888964778"
      },
      "source": [
        "# Retornando qual a posição de uma palavra dentre as outras palavras da frase\n",
        "# Para isso vamos reaproveitar a função que já criamos para identificar índice de letras\n",
        "encontra_ind('lazy', frase.split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ8EIVJU71iy"
      },
      "source": [
        "# Função para encontrar palavra em uma determinada posição da frase a nível de palavras\n",
        "def encontra_palavra(frase, ind):\n",
        "  return frase.split()[ind]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GqtabJOi-oZi",
        "outputId": "9678c34b-c2cf-4ca1-bddb-7bbd67ad2252"
      },
      "source": [
        "encontra_palavra(frase, 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'fox'"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxqLO-R871oc"
      },
      "source": [
        "# Função que concatena a primeira e a última palavra do texto\n",
        "def concat_palavras(texto):\n",
        "    palavras = texto.split()\n",
        "    pp = palavras[0]\n",
        "    up = palavras[len(palavras)-1]\n",
        "    return pp + up"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "DPM5QZ-j_PDC",
        "outputId": "e7025325-95dd-46b7-ed3b-6f37536cc972"
      },
      "source": [
        "concat_palavras(frase)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Thedog'"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    }
  ]
}