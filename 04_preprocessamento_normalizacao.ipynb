{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4. Pré-processamento e normalização.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "K9FTB2n_ALXI",
        "yXgKuBCyOuex",
        "jhUo74ouQCwm",
        "HPC-FJiKT-nM",
        "SE3NyZyB_57x",
        "aGDnGpFEck0N",
        "U0N1xq4mLjLB",
        "D4xR-cpaTNsc",
        "sEXD6jJMd5sT",
        "7LXmhIp_hYtV"
      ],
      "authorship_tag": "ABX9TyOtfMz2/MYvvbMML3FoUCvQ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9FTB2n_ALXI"
      },
      "source": [
        "# Tarefas de pré-processamento\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WpBrkYYKZo6"
      },
      "source": [
        "O pipeline clássico de PLN prevê fundamentalmente três passos na etapa de pré-processamento:\n",
        "\n",
        "1. **Tokenization -** refere-se ao procedimento de dividir uma frase em suas partes constituintes - as palavras e a pontuação das quais ela é composta. É diferente de simplesmente dividir a frase em espaços em branco e, em vez disso, divide a frase em palavras constituintes, números (se houver) e pontuação, que nem sempre podem ser separados por espaços em branco.\n",
        "\n",
        "2. **PoS (Part-of-Speech) Tagging -** é responsável pelo processo de definição da classe gramatical das palavras, de acordo com as funções sintáticas. Em PLN, o termo PoS se refere a classes gramaticais. PoS Tagging se refere ao processo de etiquetação de palavras dentro de frases com seus respectivos PoS. Extraímos os PoS de tokens que constituem uma frase para que possamos filtrar os PoS de interesse e analisá-los. PoS tagging é realizada usando diferentes técnicas, uma das quais é uma abordagem baseada em regras que constrói uma lista para atribuir uma possível tag para cada palavra.\n",
        "\n",
        "3. **Remoção de stop words -** stop words são as palavras que ocorrem com mais frequência em qualquer idioma e são usadas apenas para apoiar a construção de frases e não contribuem em nada para a semântica de uma frase. Portanto, podemos remover essas palavras de qualquer texto antes de um processo de PLL, pois elas ocorrem com muita frequência e sua presença não tem muito impacto no sentido de uma frase. Removê-las nos ajudará a limpar nossos dados, tornando sua análise muito mais eficiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXgKuBCyOuex"
      },
      "source": [
        "## 1. Tokenização"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-lpC4PGPCff"
      },
      "source": [
        "O NLTK fornece um método chamado word_tokenize, que transforma o texto em tokens. Ele separa o texto em palavras diferentes com base na pontuação e espaços entre as palavras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2jd8fVSOx-Q"
      },
      "source": [
        "# Importando o módulo necessário da NLTK\n",
        "from nltk import word_tokenize,download"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAnuJ5_nPPId",
        "outputId": "2190d42b-de88-4c75-f152-c2642f5947b3"
      },
      "source": [
        "# Baixando os corpora necessários para esse notebook\n",
        "download(['punkt','averaged_perceptron_tagger','stopwords'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouoTCyryPh7Z"
      },
      "source": [
        "# Definindo função para obtenção de tokens\n",
        "def get_tokens(sentenca):\n",
        "  palavras = word_tokenize(sentenca)\n",
        "  return palavras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Edidb00BP2pX",
        "outputId": "997e9f02-57ac-4555-88d4-20c31e35b905"
      },
      "source": [
        "# Exemplo de uso da função recém-definida\n",
        "print(get_tokens(\"Esse é um notebook sobre PLN!\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Esse', 'é', 'um', 'notebook', 'sobre', 'PLN', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhUo74ouQCwm"
      },
      "source": [
        "## 2. PoS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7PZPoMGQFO9"
      },
      "source": [
        "Vamos pensar num exemplo de PoS Taggins usando a sentença \"The sky is blue\" com as seguintes tags disponíveis:\n",
        "* DT = Determiner\n",
        "* NN = Noun, common, singular or mass\n",
        "* VBZ = Verb, present tense, third-person singular\n",
        "* JJ = Adjective\n",
        "\n",
        "Nossa sentença após o processo de PoS poderia ser representada pelo seguinte conjunto de tuplas:\n",
        "\n",
        "* (The, DT), (sky, NN), (is, VBZ), (blue, JJ)\n",
        "\n",
        "Da mesma forma que o NLTK possui um método que nos auxilia na tokenização das palavras, ele também possui um método voltado ao tageamento, o  pos_tag. Temos ao todo 34 tags disponíveis na NLTK, elas podem ser consultadas na seguinte página: https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3gHGX3YSGMT"
      },
      "source": [
        "# Importando o método\n",
        "from nltk import pos_tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEaGw6ZrSeSG"
      },
      "source": [
        "# Vamos passar uma sentença pelo método get_tokens que desenvolvemos\n",
        "tokens = get_tokens(\"This is an NLP notebook!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIorECWrP-u9"
      },
      "source": [
        "# Definindo uma função de PoS Tagging\n",
        "def get_tags(tokens):\n",
        "  return pos_tag(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rn5DUL-S96N",
        "outputId": "475522e3-23da-413e-8a12-cdf27cda9dde"
      },
      "source": [
        "# Verificando o tageamento obtido\n",
        "get_tags(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('This', 'DT'),\n",
              " ('is', 'VBZ'),\n",
              " ('an', 'DT'),\n",
              " ('NLP', 'JJ'),\n",
              " ('notebook', 'NN'),\n",
              " ('!', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPC-FJiKT-nM"
      },
      "source": [
        "## 3. Remoção de stopwords\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXUcBtz6V02y"
      },
      "source": [
        "Para realizar a remoção de stopwords podemos usar um corpus já integrado ao NLTK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcyywFMjV0OJ"
      },
      "source": [
        "# Importando dependências\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu6p1eA6VxOk",
        "outputId": "72b56856-37de-4e5c-e1c8-e03ff7182132"
      },
      "source": [
        "# Pegando as stop words da língua inglesa\n",
        "stop_words = stopwords.words('english')\n",
        "print(stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfLcii2gYZ5v",
        "outputId": "d49005ae-67e4-415b-9988-53b90db7af16"
      },
      "source": [
        "# Verificando a quantidade de stop words\n",
        "print(len(stop_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrsBQpxCYgrY"
      },
      "source": [
        "# Definindo função para remover stop words\n",
        "def rem_stopwords(sentenca, stop_words):\n",
        "  return ' '.join([palavra for palavra in sentenca if palavra not in stop_words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhIgGXTZaZs4"
      },
      "source": [
        "# Obtendo os tokens de uma sentença\n",
        "sentenca = \"Today we are learning about tokens and stop words\"\n",
        "tokens = get_tokens(sentenca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_L4u9iwSatza",
        "outputId": "583cd8e5-0182-4f23-f05a-e7a0a9b0a6eb"
      },
      "source": [
        "# Obtendo os tokens após remoção de stop words\n",
        "rem_stopwords(tokens, stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Today learning tokens stop words'"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Uk33XiIazS-",
        "outputId": "24b25223-4589-4c92-b870-d9e0963b3b72"
      },
      "source": [
        "# Podemos ainda remover ou extender stop words caso seja necessário\n",
        "stop_words.extend(\"we\")\n",
        "print(rem_stopwords(tokens, stop_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Today we learning tokens stop words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE3NyZyB_57x"
      },
      "source": [
        "# Normalização"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfbapXiH_7ko"
      },
      "source": [
        "O processo de **normalização de palavras** refere-se ao processo de redução/simplificação de palavras, buscando obter seus radicais. De maneira mais precisa a normalização de textos é um processo em que diferentes variações de texto são convertidas em um formato padrão. Precisamos realizar a normalização do texto, pois há algumas palavras que podem significar a mesma coisa.\n",
        "\n",
        "Três técnicas muito importantes que envolvem a normalização são:\n",
        "\n",
        "* **Spelling Correction** - Primeiro,identificamos a palavra com grafia incorreta, o que pode ser feito por pesquisa no dicionário. Se não houver correspondência no dicionário será considerado incorreto. Depois substituímos pela palavra soletrada corretamente. Existem muitos algoritmos para essa tarefa, como o algoritmo de distância de edição mínima, que escolhe a palavra soletrada corretamente mais próxima para uma palavra com erros ortográficos.\n",
        "\n",
        "* **Stemming** - Reduzir palavras flexionadas/derivadas ao seu tronco/base, não necessariamente reduzindo à raíz morgfológica. Geralmente refere-se a um processo de heurística que corta as extremidades das palavras.\n",
        "\n",
        "* **Lemmatization** - Consiste em aplicar técnicas para deflexionar uma palavra para uma forma padrão, seja alterando gênero, número ou conjugação. É como se estivessemos reduzindo uma palavra para sua forma de dicionário.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGDnGpFEck0N"
      },
      "source": [
        "## 4 Exemplo básico de normalização"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbtz4sf2ckYA"
      },
      "source": [
        "# Sentença que trabalharemos em cima\n",
        "sentenca = \"Eu visitei os EUA depois de ter ido no RU em 22-10-18\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaDgITEGKLlI"
      },
      "source": [
        "# Função para normalizar especificamente a sentença definida\n",
        "def normalizar(texto):\n",
        "  return texto.replace(\"EUA\", \"Estados Unidos da América\").replace(\"RU\", \"Reino Unido\").replace(\"-18\", \"-2018\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n74_alJfLP-x",
        "outputId": "090a7fed-e19d-40b4-818c-55e23f48fd3c"
      },
      "source": [
        "sentenca_normalizada = normalizar(sentenca)\n",
        "print(sentenca_normalizada)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eu visitei os Estados Unidos da América depois de ter ido no Reino Unido em 22-10-2018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0N1xq4mLjLB"
      },
      "source": [
        "## 5. Correção ortográfica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orlHkU3FLb3a",
        "outputId": "62149d5c-92f8-4499-cb25-f3db465d3bc4"
      },
      "source": [
        "# Vamos instalar a biblioteca de correção ortográfica\n",
        "!pip install autocorrect"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.5.0.tar.gz (622 kB)\n",
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 25.3 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 30 kB 22.9 MB/s eta 0:00:01\r\u001b[K     |██                              | 40 kB 18.7 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 61 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 71 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 92 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 102 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 112 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 122 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 133 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 143 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████                        | 153 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 163 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 174 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 184 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 194 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 204 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 215 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 225 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 235 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 245 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 256 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 266 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 276 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 286 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 296 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 307 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 317 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 327 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 337 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 348 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 358 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 368 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 378 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 389 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 399 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 409 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 419 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 430 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 440 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 450 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 460 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 471 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 481 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 491 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 501 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 512 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 522 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 532 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 542 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 552 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 563 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 573 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 583 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 593 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 604 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 614 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 622 kB 7.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.5.0-py3-none-any.whl size=621851 sha256=508bf7eeb2ce62fa4769751cc3612a8e5199ddd305b2423580185112c9ccea7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/8e/bd/f6fd900a056a031bf710a00bca338d86f43b83f0c25ab5242f\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR-Z6djXMISS"
      },
      "source": [
        "# Importando bibliotecas que usaremos\n",
        "import nltk\n",
        "from nltk import word_tokenize  \n",
        "from autocorrect import Speller"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yZQNegn0MPVf",
        "outputId": "9046e5c2-54fe-499d-c0df-bf52a630822d"
      },
      "source": [
        "# Definindo objeto de corretor da língua inglesa e testando ele\n",
        "spell = Speller(lang='en')\n",
        "spell('Sensateonal')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Sensational'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioIjhNBERVrt",
        "outputId": "1599af04-5aed-4945-97bb-ec06740f60a2"
      },
      "source": [
        "# Baixando o tokenizador de sentenças\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EhU4KG4Makq"
      },
      "source": [
        "# Sentença que testaremos o speller\n",
        "sentenca_tks = word_tokenize(\"Ntural Luanguage Processin deals with the art of extracting insightes from Natural Languaes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLMIfevtO7PL"
      },
      "source": [
        "# Definindo função para correção ortográfica\n",
        "def corretor_ort(tokens):\n",
        "  sentenca_correta = \" \".join([spell(word) for word in tokens])\n",
        "  return sentenca_correta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "SJ4DtwGySM9R",
        "outputId": "a313ed5f-c55c-4038-d776-0ba9414bba9d"
      },
      "source": [
        "# Verificando se sentença foi corrigida\n",
        "corretor_ort(sentenca_tks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Natural Language Processing deals with the art of extracting insights from Natural Languages'"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4xR-cpaTNsc"
      },
      "source": [
        "## 6. Stemming\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCeYuTzjVfu_"
      },
      "source": [
        "# Importando a ferramenta de stemming da NLTK\n",
        "from nltk import stem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJYz-n8PVm1r"
      },
      "source": [
        "# Definição de função que retorna o stem de uma palavra usando um determinado algoritmo\n",
        "def get_stem(palavra, stemmer):\n",
        "  return stemmer.stem(palavra)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq6FGsqDbL0m"
      },
      "source": [
        "# Criando objeto stemmer usando o algoritmo de Porter\n",
        "porter_stem = stem.PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk8UYNUYbDEi",
        "outputId": "4ea3d12e-4234-4fac-b27e-16247ea0c9a8"
      },
      "source": [
        "# Verificando funcionamento do stemming com alguns exemplos\n",
        "print(get_stem(\"production\",porter_stem))\n",
        "print(get_stem(\"coming\",porter_stem))\n",
        "print(get_stem(\"firing\",porter_stem))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "product\n",
            "come\n",
            "fire\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ACjKusKeCwF",
        "outputId": "d3cbbd28-3cb1-4c65-905f-37ef50753138"
      },
      "source": [
        "# Um exemplo onde obtemos um stem que não é uma palavra\n",
        "print(get_stem(\"battling\",porter_stem))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "battl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEXD6jJMd5sT"
      },
      "source": [
        "## 7. Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdLy8MwyeR9n",
        "outputId": "2898cd2b-736e-470a-a3f2-0bf3142bca92"
      },
      "source": [
        "# Importando dependências necessárias\n",
        "from nltk import download\n",
        "download(\"wordnet\")\n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTrvz6k3fdYX"
      },
      "source": [
        "# Definindo objeto lematizador\n",
        "lematizador = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n517GRigSFr"
      },
      "source": [
        "# Definindo função que pega uma palavra e devolve seu lema\n",
        "def get_lemma(palavra):\n",
        "  return lematizador.lemmatize(palavra)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LzrrYCtgnv9",
        "outputId": "91ba3969-5225-4cfa-b3ce-461f4f0520a0"
      },
      "source": [
        "# Testando alguns usos do lematizador\n",
        "print(get_lemma('products'))\n",
        "print(get_lemma('production'))\n",
        "print(get_lemma('battles'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "product\n",
            "production\n",
            "battle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LXmhIp_hYtV"
      },
      "source": [
        "## 8. Detecção de sentença"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORdlltnuhdsV"
      },
      "source": [
        "# Importando dependências, no caso a função tokenizadora de sentenças\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjewUyETiy0j"
      },
      "source": [
        "# Definindo função separadora de sentenças\n",
        "def get_sentencas(texto):\n",
        "  return sent_tokenize(texto)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_I66DKJjARh",
        "outputId": "89731f1b-2ceb-4b6c-c554-639a01484fbe"
      },
      "source": [
        "# Testando a função criada\n",
        "get_sentencas(\"Esse é um grande dia. De fato, um dos melhores! Me sinto muito bem.\")\n",
        "# Veja que a função delimita as sentenças por pontuação"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Esse é um grande dia.', 'De fato, um dos melhores!', 'Me sinto muito bem.']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCH-YFfWj-Tq",
        "outputId": "272ae468-54cf-4d66-cc30-fb8ffe4a7e4f"
      },
      "source": [
        "# Nossa função é capaz de diferenciar pontos de final de frase e de abreviação\n",
        "get_sentencas(\"Estamos no consultório do Dr. Marcos. Só nos resta esperar\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Estamos no consultório do Dr. Marcos.', 'Só nos resta esperar']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}