{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5. N-gramas e modelos de linguagem.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cTO6T9hxmAhS",
        "SWPXNKg_jHH7",
        "vu7pPglPlfB_",
        "hSf52I8EosQ1",
        "LfiCY2uH1ntt"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTO6T9hxmAhS"
      },
      "source": [
        "# N-gramas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNvmRzhymvy6"
      },
      "source": [
        "Um **n-grama** é uma sequência contígua de n elementos de um determinado texto. Esses elementos podem ser fonemas, sílabas, palavras ou tokens.\n",
        "\n",
        "N-gramas podem ser úteis na atividade de prever a n-ésima palavra se baseando nas n-1 palavras anteriores. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNg-R9KLkEYE"
      },
      "source": [
        "# Importando dependências necessárias\n",
        "import re\n",
        "from nltk import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOsbTFMwpzIu"
      },
      "source": [
        "# Definindo função de extração de n-gramas\n",
        "# Veja que n é um parâmetro da função\n",
        "def ext_n_gram(sentenca, n):\n",
        "  tokens = word_tokenize(sentenca)\n",
        "  for i in range(len(tokens)-n+1):\n",
        "    print(tokens[i:i+n])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTiITesVuAmY",
        "outputId": "f0dd7309-0490-44d5-ce56-625d13974773"
      },
      "source": [
        "# Testando para 2-gramas\n",
        "ext_n_gram(\"The dog and the cat are playing togheter.\", 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'dog']\n",
            "['dog', 'and']\n",
            "['and', 'the']\n",
            "['the', 'cat']\n",
            "['cat', 'are']\n",
            "['are', 'playing']\n",
            "['playing', 'togheter']\n",
            "['togheter', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJJMUiNfubY0",
        "outputId": "a7fcfd07-7444-489d-9a12-35e2373e124c"
      },
      "source": [
        "# Testando para 3-gramas\n",
        "ext_n_gram(\"The dog and the cat are playing togheter.\", 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'dog', 'and']\n",
            "['dog', 'and', 'the']\n",
            "['and', 'the', 'cat']\n",
            "['the', 'cat', 'are']\n",
            "['cat', 'are', 'playing']\n",
            "['are', 'playing', 'togheter']\n",
            "['playing', 'togheter', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vieYsForuh7O"
      },
      "source": [
        "# Vamos agora usar o nltk ao invés de fazer a separação à mão\n",
        "from nltk import ngrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hV7nhwYMusxD",
        "outputId": "23e80374-c4f8-4d74-bfc4-23d031159710"
      },
      "source": [
        "# Uso da função importada\n",
        "list(ngrams('The dog and the cat are playing togheter.'.split(), 2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'dog'),\n",
              " ('dog', 'and'),\n",
              " ('and', 'the'),\n",
              " ('the', 'cat'),\n",
              " ('cat', 'are'),\n",
              " ('are', 'playing'),\n",
              " ('playing', 'togheter.')]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdF_vZZqvJPS",
        "outputId": "f8e2ba7c-ee8c-4b52-dc57-2a1fc40016f4"
      },
      "source": [
        "# Vamos usar uma outra biblioteca com função semelhante a da nltk, a textbloob\n",
        "!pip install -U textblob\n",
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV_xelIkzpC4"
      },
      "source": [
        "# Frase que usaremos\n",
        "blob = TextBlob(\"The dog and the cat are playing togheter.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xox1GPoPxUDQ",
        "outputId": "c827205f-0d4f-49df-d485-6e7dba1935a1"
      },
      "source": [
        "# Usando a textblob para 2-gramas\n",
        "blob.ngrams(n=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['The', 'dog']),\n",
              " WordList(['dog', 'and']),\n",
              " WordList(['and', 'the']),\n",
              " WordList(['the', 'cat']),\n",
              " WordList(['cat', 'are']),\n",
              " WordList(['are', 'playing']),\n",
              " WordList(['playing', 'togheter'])]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rn4_M9Yy4qP",
        "outputId": "7345ec75-127e-40d1-cf34-6db99841496e"
      },
      "source": [
        "# Usando a textblob para 3-gramas\n",
        "blob.ngrams(n=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['The', 'dog', 'and']),\n",
              " WordList(['dog', 'and', 'the']),\n",
              " WordList(['and', 'the', 'cat']),\n",
              " WordList(['the', 'cat', 'are']),\n",
              " WordList(['cat', 'are', 'playing']),\n",
              " WordList(['are', 'playing', 'togheter'])]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVBRUqQ3zwHu"
      },
      "source": [
        "# Modelos de linguagem com n-gramas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rJyZi7Tz1fB"
      },
      "source": [
        "Como estavamos dizendo, n-gramas podem ser úteis para prever a n-ésima palavra baseando-se apenas na n-1 palavras anteriores. Modelos usados para prever uma determinada sequência de palavras são chamados de **modelos de linguagem**. \n",
        "\n",
        "O objetivo principal dos modelos de linguagem é atribuir uma probabilidade a uma determinada sentença ou sequência de palavras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWPXNKg_jHH7"
      },
      "source": [
        "## 1. Funções que usaremos "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZmgWVMW1xyx"
      },
      "source": [
        "# Vamos começar importando as dependências necessárias\n",
        "\n",
        "# Expressões regulares\n",
        "import re\n",
        "\n",
        "# Contador de frequência de aparições em um iterable\n",
        "from collections import Counter\n",
        "\n",
        "# Trabalhar com partes de um iterable\n",
        "from itertools import islice\n",
        "\n",
        "# Capacidade de trabalhar com geradores pseudo-aleatórios\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEddizyo2T1G"
      },
      "source": [
        "# Definindo a regex que usaremos para encontrar palavras\n",
        "regex = r\"[-'a-zA-ZÀ-ÖØ-öø-ÿ]+\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UUudI393VNP"
      },
      "source": [
        "# Definindo função para encontrar tokens de um arquivo\n",
        "def get_tokens(nome_arq):\n",
        "  with open(nome_arq, 'r') as documento:\n",
        "    # Pegando vetor com todas as linhas do arquivo\n",
        "    conteudo = documento.read()\n",
        "    # Colocando todas as letras do arquivo em minúsculo\n",
        "    conteudo = conteudo.lower()\n",
        "  tokens = re.findall(regex, conteudo)\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmakIj7C4bQ9"
      },
      "source": [
        "# Definindo função que pega tokens que não sejam stopwords\n",
        "def get_tokens_s_stopwords(nome_arq, stopwords = \"/content/stopwords_pt.txt\"):\n",
        "  # Pegando o texto\n",
        "  with open(nome_arq, 'r') as documento:\n",
        "    conteudo = documento.read()\n",
        "    conteudo = conteudo.lower()\n",
        "  # Pegando as stopwords\n",
        "  with open(stopwords, 'r') as stopwords_arq:\n",
        "    stopwords = set([])\n",
        "    for s in stopwords_arq.readline():\n",
        "      stopwords.add(s.strip().lower())\n",
        "  # Removendo as stopwords\n",
        "  tokens = [palavra for palavra in re.findall(regex,conteudo) if w not in stopwords]\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFU0LQAQ75Nz"
      },
      "source": [
        "# Função de janela deslizante que retorna uma janela de tamanho n\n",
        "# A cada chamada dessa função ela se move para o próxima elemento, por isso deslizante\n",
        "def janela(seq, n=2):\n",
        "  # Define iter como iterável sobre seq\n",
        "  it = iter(seq)\n",
        "  # O resultado é uma tupla que possui a primeira janela de tamanho n sobre iter\n",
        "  resultado = tuple(islice(it,n))\n",
        "  # Verifica se existem elementos suficientes para construir a primeira janela de tamanho n\n",
        "  if len(resultado) == n:\n",
        "    yield resultado\n",
        "  # É aqui onde a ação de deslizar a janela ocorre, atualizando resultado com o próximo elemento\n",
        "  for elem in it:\n",
        "    resultado = resultado[1:] + (elem,)\n",
        "    yield resultado\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JVgjlOi_cNB"
      },
      "source": [
        "# Definição da função que retorna a frequência de bigramas e unigramas\n",
        "def freq_ngramas(palavras):\n",
        "  unigramas = Counter(palavras)\n",
        "  bigramas = Counter(janela(palavras, 2))\n",
        "  return (unigramas, bigramas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_DYmIoCii4v"
      },
      "source": [
        "# Função lambda para calcular as probabilidades baseando-se nas contagens\n",
        "# A probabilidade a ser calculada é a p(w2) dada uma palavra predecessora w1\n",
        "# Aqui, unigrmas e bigramas referem-se aos objetos criados na função freq_ngramas\n",
        "prob_bigram = lambda p1,p2: bigramas[(p1, p2)]/ unigramas[p1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xRtq5BTjeET"
      },
      "source": [
        "# Função que pega uma série de frases e aplica um score de probabilidade delas serem escolhidas\n",
        "def score(frases):\n",
        "  for frase in frases:\n",
        "    palavras = re.findall(regex, frase)\n",
        "    p = float(1.0)\n",
        "    for p1, p2 in janela(palavras, 2):\n",
        "      p = p * prob_bigram(p1,p2)\n",
        "    print(f\"Frase '{frase}' possui p = {p:.20f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu7pPglPlfB_"
      },
      "source": [
        "## 2. Testando o modelo criado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCl4_OMHlinJ"
      },
      "source": [
        "# Sentenças que usaremos para testar o modelo que desenvolvemos com as funções\n",
        "sentencas = [\"uma noite destas, vindo da cidade\",\n",
        "             \"uma noite destas, vindo da\", \n",
        "             \"uma noite destas, vindo\", \n",
        "             \"uma noite destas\", \n",
        "             \"uma noite\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r2nNhwzlu7_"
      },
      "source": [
        "# Vamos calcular a probabilidade de cada sentença, nos baseando num livro de Machado de Assis\n",
        "tokens = get_tokens(\"/content/dom_casmurro.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1olKUasbnwvX"
      },
      "source": [
        "# Formação de unigramas e bigramas usando os tokens capturados\n",
        "unigramas, bigramas = freq_ngramas(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AfyPlc2qjAp",
        "outputId": "8a39435b-7ebc-4f92-d9a7-ebc9a607a7bb"
      },
      "source": [
        "# Verificando os scores de cada sentença\n",
        "score(sentencas)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frase 'uma noite destas, vindo da cidade' possui p = 0.00000005316696327186\n",
            "Frase 'uma noite destas, vindo da' possui p = 0.00000477743141400026\n",
            "Frase 'uma noite destas, vindo' possui p = 0.00004299688272600236\n",
            "Frase 'uma noite destas' possui p = 0.00021498441363001179\n",
            "Frase 'uma noite' possui p = 0.00902934537246049586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cOEqeLToUWP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73091887-c97a-4ea7-ff13-82872786ac66"
      },
      "source": [
        "# Vendo a quantidade de tokens, unigramas e bigramas\n",
        "print(len(tokens))\n",
        "print(len(unigramas))\n",
        "print(len(bigramas))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68895\n",
            "10352\n",
            "44495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSf52I8EosQ1"
      },
      "source": [
        "## 3. Usando bigramas para descobrir próxima palavra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgrqNbWUthT6"
      },
      "source": [
        "# Definindo função que indica as mais provavéis próxima palavra para uma palavra predecessora\n",
        "def prob_prox(frase, n=5):\n",
        "  # Quebra da frase em tokens\n",
        "  tokens = re.findall(regex, frase)\n",
        "  # Calcula as probabilidades de todas as palavras em que o bigrama começa por w1 e armazena em uma lista\n",
        "  probs = {w2 : prob_bigram(w1, w2) for (w1, w2) in bigramas.keys() if w1 == tokens[-1]}\n",
        "  # Ordenamos e mostramos os n mais prováveis\n",
        "  for palavra, probabilidade in islice(sorted(probs.items(), \n",
        "                                       key = lambda item: item[1], \n",
        "                                       reverse = True), n):\n",
        "    print(f'{frase} + {palavra} -> probabilidade = {probabilidade*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrVP9zVouZXS"
      },
      "source": [
        "# Vamos testar a função definida acima com o corpus de dom casmurro\n",
        "tokens = get_tokens('/content/dom_casmurro.txt')\n",
        "unigramas, bigramas = freq_ngramas(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78UmR6IR1Kqw",
        "outputId": "d8d49009-389c-4970-8bd9-91510ad4e35b"
      },
      "source": [
        "# Testando com um input do usuário\n",
        "frase = input(\"\\nDigite uma frase: \")\n",
        "prob_prox(frase)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Digite uma frase: quando\n",
            "quando + eu -> probabilidade = 10.81%\n",
            "quando + me -> probabilidade = 4.86%\n",
            "quando + elle -> probabilidade = 4.86%\n",
            "quando + lhe -> probabilidade = 3.78%\n",
            "quando + a -> probabilidade = 3.78%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfiCY2uH1ntt"
      },
      "source": [
        "## 4. Criando classe NGramas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZpCINtC2U9V"
      },
      "source": [
        "Vamos criar uma classe NGramas que incorpore as funções de modelagem que desenvolvemos até agora."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENX9QJ6Z2bZO"
      },
      "source": [
        "class NGramas(object):\n",
        "\n",
        "  # O construtor da classe aceita um número máximo para n e o objeto com as palavras\n",
        "  def __init__(self, max_n, palavras = None):\n",
        "    self.max_n   = max_n\n",
        "    # Lembrando que Counter() é uma collection, subclasse de dicionário, usada para contar elementos que podem ser representados por hash\n",
        "    self.contador  = Counter()\n",
        "    if palavras is not None:\n",
        "      self.update(palavras)\n",
        "\n",
        "  # Método que, baseado nas palavras, encontra e conta os unigramas, bigramas, ..., n-gramas\n",
        "  def update(self, palavras):\n",
        "    # Realizando a contagem para cada valor de n usando a função janela\n",
        "    for i in range(1,self.max_n+1):\n",
        "        self.contador.update(janela(palavras,i))\n",
        "    # Abaixo temos um caso especial onde a tupla de janela retorna vazia\n",
        "    self.contador[()] += len(palavras)\n",
        "\n",
        "  # Calcula as probabilidades para a frase usando a lista de tokens/palavras\n",
        "  # Usa estratégia recursiva que vai até o 1-grama para ir retornando até os n-gramas\n",
        "  def probabilidades(self, palavras):\n",
        "      # Testando se existem palavras para compor o ngrama\n",
        "      if len(palavras) <= self.max_n:\n",
        "          return self.probabilidades(palavras)\n",
        "      else:\n",
        "          P = 1\n",
        "          for i in range(len(palavras) - self.max_n + 1):\n",
        "              ngram = palavras[i:i + self.max_n]\n",
        "              P     = P * self.probabilidade(ngram)\n",
        "          return P\n",
        "  \n",
        "  # Calcula a probabilidade de determinado n-grama usando seu prefixo\n",
        "  def probabilidade(self, ngram):\n",
        "      # Cria ngrama como tupla \n",
        "      ngram = tuple(ngram)\n",
        "      ngram_count = self.Counts[ngram]\n",
        "      prefix_count = self.Counts[ngram[:-1]]\n",
        "      # Se uma tupla (n-grama) nao for observada devolvemos zero\n",
        "      if ngram_count and prefix_count:\n",
        "          return ngram_count / prefix_count\n",
        "      else:\n",
        "          return 0.0\n",
        "      # Geracao de frases de 'n_words'\n",
        "  def generate(self, n_words, threshold = random.random()):\n",
        "      \n",
        "      # cria uma lista de unigrams\n",
        "      unigrams = [ngram for ngram in self.Counts.keys() if len(ngram) == 1]\n",
        "      # Tentamos gerar frases \n",
        "      words = []\n",
        "      \n",
        "      while len(words) < n_words:\n",
        "          # o prefixo para o proximo n-grama\n",
        "          if self.max_n == 1:\n",
        "              prefix = ()\n",
        "          else:\n",
        "              prefix = tuple(words[-self.max_n + 1:])\n",
        "         \n",
        "          total     = 0.0\n",
        "          random.shuffle(unigrams)\n",
        "          for unigram in unigrams:\n",
        "              total += self._probability(prefix + unigram)\n",
        "              if total >= threshold:\n",
        "                  words.extend(unigram)\n",
        "                  break\n",
        "          \n",
        "          # Se nao for possivel criar uma frase  \n",
        "          if total == 0.0:\n",
        "              raise RuntimeError('impossible sequence')\n",
        "      return(words)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}